import streamlit as st
import os
import json
from dotenv import load_dotenv
from typing import TypedDict, List, Any
import numpy as np
import pandas as pd

from langchain_upstage import ChatUpstage
from langchain.schema.messages import HumanMessage, SystemMessage, AIMessage
from langgraph.graph import StateGraph, END

from rag.retriever import VectorBasedRecommender
from rag.nodes import (
    llm_parser_node,
    similar_node,
    vibe_node,
    hybrid_node,
    general_node,
    route_by_mode,
    generate_response_node,
    game_name_normalizer_node
)

# --- ì´ˆê¸°í™” --- #
load_dotenv()

UPSTAGE_API_KEY = os.environ.get("UPSTAGE_API_KEY")
if not UPSTAGE_API_KEY:
    st.error("Upstage API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. .env íŒŒì¼ì— UPSTAGE_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()

@st.cache_resource
def init_recommender():
    data_path = os.path.join(os.path.dirname(__file__), 'data')
    # Allow overriding the model from an environment variable or use a new default
    embedding_model = os.environ.get("UPSTAGE_EMBEDDING_MODEL", "solar-embedding-1-large")
    return VectorBasedRecommender(data_path=data_path, embedding_model=embedding_model)

@st.cache_resource
def init_llm():
    # ChatOpenAI ëŒ€ì‹  ChatUpstage ì‚¬ìš©
    return ChatUpstage(api_key=UPSTAGE_API_KEY)

recommender = init_recommender()
llm = init_llm()

# --- LangGraph ìƒíƒœ ë° ë…¸ë“œ ì •ì˜ --- #

class GraphState(TypedDict):
    user_query: str
    parsed_json: dict
    rerank_weights: dict
    candidate_appids: List[int]
    query_vector: np.ndarray
    final_results: Any # Can be DataFrame or final string

# ê° ë…¸ë“œì— ë¦¬ì†ŒìŠ¤(llm, recommender)ë¥¼ ì£¼ì…í•˜ëŠ” ë˜í¼ í•¨ìˆ˜
def build_parser_node(state): return llm_parser_node(state, llm)
def build_normalizer_node(state): return game_name_normalizer_node(state, recommender)
def build_similar_node(state): return similar_node(state, recommender)
def build_vibe_node(state): return vibe_node(state, recommender)
def build_hybrid_node(state): return hybrid_node(state, recommender)
def build_response_generator_node(state): return generate_response_node(state, llm)

def rerank_node(state: GraphState):
    appids = state['candidate_appids']
    query_vec = state['query_vector']
    weights = state['rerank_weights']
    reranked_df = recommender.rerank_candidates(appids, query_vec, weights, top_n=5)
    state['final_results'] = reranked_df
    return state

# --- ê·¸ë˜í”„ ë¹Œë“œ --- #
workflow = StateGraph(GraphState)
workflow.add_node("parser_node", build_parser_node)
workflow.add_node("normalizer_node", build_normalizer_node)
workflow.add_node("similar_node", build_similar_node)
workflow.add_node("vibe_node", build_vibe_node)
workflow.add_node("hybrid_node", build_hybrid_node)
workflow.add_node("rerank_node", rerank_node)
workflow.add_node("general_node", general_node)
workflow.add_node("response_generator_node", build_response_generator_node)

workflow.set_entry_point("parser_node")
workflow.add_edge("parser_node", "normalizer_node")
workflow.add_conditional_edges(
    "normalizer_node", 
    route_by_mode,
    {
        "similar_node": "similar_node", "vibe_node": "vibe_node",
        "hybrid_node": "hybrid_node", "general_node": "general_node",
    }
)

workflow.add_edge('similar_node', 'rerank_node')
workflow.add_edge('vibe_node', 'rerank_node')
workflow.add_edge('hybrid_node', 'rerank_node')
workflow.add_edge('rerank_node', 'response_generator_node')
workflow.add_edge('general_node', END)
workflow.add_edge('response_generator_node', END)

app_graph = workflow.compile()

# --- Streamlit UI --- #
st.title("âœ¨ ê²Œì„ ì¶”ì²œ ì„œë¹„ìŠ¤")

with st.sidebar:
    st.header("ì¬ì •ë ¬ ê°€ì¤‘ì¹˜ ì„¤ì •")
    st.caption("ê° ìš”ì†Œì˜ ì¤‘ìš”ë„ë¥¼ 0~10ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.")
    user_weights = {
        "tag_match": st.slider("TagMatch (ì¿¼ë¦¬-ê²Œì„ ìœ ì‚¬ë„)", 0, 10, 8),
        "novelty": st.slider("Novelty (ìƒˆë¡œì›€)", 0, 10, 2),
    }

if "messages" not in st.session_state:
    st.session_state.messages = [AIMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ê²Œì„ì„ ì¶”ì²œí•´ë“œë¦´ê¹Œìš”?")]

for msg in st.session_state.messages:
    if isinstance(msg, AIMessage):
        st.chat_message("assistant").write(msg.content)
    elif isinstance(msg, HumanMessage):
        st.chat_message("user").write(msg.content)
    elif isinstance(msg, SystemMessage):
        st.chat_message("assistant").write(msg.content)

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append(HumanMessage(content=prompt))
    st.chat_message("user").write(prompt)

    with st.spinner("ì¶”ì²œ ì¤‘..."):
        status = st.empty()
        json_container = st.empty()
        
        try:
            graph_input = {"user_query": prompt, "rerank_weights": user_weights}
            
            with st.status("ì¶”ì²œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...", expanded=True) as status_container:
                executed_nodes = []
                latest_state = None

                for event in app_graph.stream(graph_input):
                    for node_name, node_state in event.items():
                        executed_nodes.append(node_name)
                        path_str = " -> ".join(f"`{node}`" for node in executed_nodes)
                        st.markdown(f"**í˜„ì¬ ì‹¤í–‰ ë…¸ë“œ:** `{node_name}`")
                        st.markdown(f"**ì „ì²´ ì‹¤í–‰ ê²½ë¡œ:** {path_str}")

                        with st.expander(f"`{node_name}` ì‹¤í–‰ ê²°ê³¼ ë³´ê¸°"):
                            if node_name == "parser_node":
                                st.markdown("LLMì´ ì‚¬ìš©ìì˜ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ì¶”ì²œ ëª¨ë“œì™€ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")
                                st.json(node_state.get('parsed_json', {}))
                            
                            elif node_name == "normalizer_node":
                                st.markdown("ì¿¼ë¦¬ì— í¬í•¨ëœ ê²Œì„ ì´ë¦„ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ìˆëŠ”ì§€ í™•ì¸í•˜ê³  í‘œì¤€í™”í•©ë‹ˆë‹¤.")
                                st.write("**:red[ë³€ê²½ ì „]**", latest_state.get('parsed_json', {}))
                                st.write("**:blue[ë³€ê²½ í›„]**", node_state.get('parsed_json', {}))

                            elif node_name in ["similar_node", "vibe_node", "hybrid_node"]:
                                st.markdown(f"`{node_name}`ì— ë”°ë¼ í›„ë³´ ê²Œì„ ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                                    # ğŸ”½ í™•ì¥ëœ íƒœê·¸ ë³´ì—¬ì£¼ê¸° (expand_query_tagsê°€ ì• ë‹¨ê³„ì—ì„œ ì‹¤í–‰ë˜ì–´ stateì— ë°˜ì˜ëœ ê²½ìš°)
                                expanded_tags = node_state.get('parsed_json', {}).get('target_tags', [])
                                if expanded_tags:
                                    st.markdown("**í™•ì¥ëœ íƒ€ê²Ÿ íƒœê·¸ (name, weight)**")
                                    st.json(expanded_tags)
                                else:
                                    st.caption("í™•ì¥ëœ íƒœê·¸ ì—†ìŒ")

                                candidate_ids = node_state.get('candidate_appids', [])
                                st.write("í›„ë³´ AppIDs:", candidate_ids)
                                if candidate_ids:
                                    df = recommender.games_df.loc[candidate_ids]
                                    st.dataframe(df[['game_title', 'tags']])

                            elif node_name == "rerank_node":
                                st.markdown("í›„ë³´ ê²Œì„ ëª©ë¡ì„ ì‚¬ìš©ìê°€ ì„¤ì •í•œ ê°€ì¤‘ì¹˜ì— ë”°ë¼ ì¬ì •ë ¬í•˜ì—¬ ìµœì¢… 5ê°œ ê²Œì„ì„ ì„ íƒí•©ë‹ˆë‹¤.")
                                st.dataframe(node_state.get('final_results'))

                            elif node_name == "response_generator_node":
                                st.markdown("ìµœì¢… ì¶”ì²œ ëª©ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì¶”ì²œì‚¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                                st.info(node_state.get('final_results'))
                            
                            elif node_name == "general_node":
                                st.markdown("ì¼ë°˜ì ì¸ ëŒ€í™”í˜• ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
                                st.info(node_state.get('final_results'))

                        latest_state = node_state
                        st.markdown("---")

                status_container.update(label="ì¶”ì²œ ì™„ë£Œ!", state="complete", expanded=False)

            final_state = latest_state

            if final_state:
                response_content = final_state.get('final_results', "ì˜¤ë¥˜: ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                
                with st.chat_message("assistant"):
                    if isinstance(response_content, pd.DataFrame):
                        st.markdown("### ìµœì¢… ì¶”ì²œ ê²Œì„ ëª©ë¡")
                        st.dataframe(response_content)
                        st.session_state.messages.append(AIMessage(content=response_content.to_markdown(index=False)))
                    else:
                        if isinstance(response_content, list):
                            response_content = '\n'.join(map(str, response_content))
                        st.markdown(response_content)
                        st.session_state.messages.append(AIMessage(content=str(response_content)))
            else:
                with st.chat_message("assistant"):
                    st.error("ì¶”ì²œì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (ê·¸ë˜í”„ê°€ ì™„ë£Œë˜ì§€ ì•ŠìŒ).")

        except Exception as e:
            st.error("ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            st.exception(e)
            response_content = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            st.session_state.messages.append(AIMessage(content=response_content))